/**
 * Workflow Execution Service
 * Handles the execution of workflows with real backend integration
 */

import { CustomNode, Edge } from '@/state/workflowStore'
import toast from 'react-hot-toast'
import { ollamaService } from './ollamaService'
import { fetchWithRetry, networkMonitor } from '@/utils/networkUtils'

export interface ExecutionProgress {
  nodeId: string
  status: 'pending' | 'running' | 'completed' | 'error'
  message?: string
  result?: any
  progress?: number
}

export interface ExecutionResult {
  workflowId: string
  status: 'success' | 'error' | 'partial'
  results: Record<string, any>
  errors?: string[]
  executionTime: number
}

export class WorkflowExecutor {
  private baseUrl: string
  private abortController: AbortController | null = null
  
  constructor(baseUrl: string = 'http://localhost:8000') {
    this.baseUrl = baseUrl
  }
  
  /**
   * Validate workflow before execution
   */
  async validateWorkflow(nodes: CustomNode[], edges: Edge[]): Promise<{ valid: boolean; errors: string[] }> {
    const errors: string[] = []
    
    // Check for at least one input node
    const inputNodes = nodes.filter(n => n.type === 'input')
    if (inputNodes.length === 0) {
      errors.push('Workflow must have at least one input node')
    }
    
    // Check for at least one output node
    const outputNodes = nodes.filter(n => n.type === 'output')
    if (outputNodes.length === 0) {
      errors.push('Workflow must have at least one output node')
    }
    
    // Check for disconnected nodes
    const connectedNodeIds = new Set<string>()
    edges.forEach(edge => {
      connectedNodeIds.add(edge.source)
      connectedNodeIds.add(edge.target)
    })
    
    const disconnectedNodes = nodes.filter(n => !connectedNodeIds.has(n.id))
    if (disconnectedNodes.length > 0 && nodes.length > 1) {
      errors.push(`${disconnectedNodes.length} node(s) are not connected`)
    }
    
    // Check for circular dependencies
    if (this.hasCircularDependency(nodes, edges)) {
      errors.push('Workflow contains circular dependencies')
    }
    
    // Validate node configurations
    nodes.forEach(node => {
      if (!node.data.isConfigured) {
        errors.push(`Node "${node.data.label}" is not configured`)
      }
      
      // Specific validation for LLM nodes
      if (node.type === 'llm') {
        const llmData = node.data as any
        if (!llmData.models?.length) {
          errors.push(`LLM node "${node.data.label}" has no model selected`)
        }
        if (!llmData.prompt?.trim()) {
          errors.push(`LLM node "${node.data.label}" has no prompt`)
        }
      }
    })
    
    return {
      valid: errors.length === 0,
      errors
    }
  }
  
  /**
   * Check for circular dependencies using DFS
   */
  private hasCircularDependency(nodes: CustomNode[], edges: Edge[]): boolean {
    const adjacencyList: Record<string, string[]> = {}
    
    // Build adjacency list
    nodes.forEach(node => {
      adjacencyList[node.id] = []
    })
    
    edges.forEach(edge => {
      if (!adjacencyList[edge.source]) {
        adjacencyList[edge.source] = []
      }
      adjacencyList[edge.source].push(edge.target)
    })
    
    // DFS to detect cycles
    const visited = new Set<string>()
    const recursionStack = new Set<string>()
    
    const hasCycle = (nodeId: string): boolean => {
      visited.add(nodeId)
      recursionStack.add(nodeId)
      
      for (const neighbor of adjacencyList[nodeId] || []) {
        if (!visited.has(neighbor)) {
          if (hasCycle(neighbor)) return true
        } else if (recursionStack.has(neighbor)) {
          return true
        }
      }
      
      recursionStack.delete(nodeId)
      return false
    }
    
    for (const nodeId of Object.keys(adjacencyList)) {
      if (!visited.has(nodeId)) {
        if (hasCycle(nodeId)) return true
      }
    }
    
    return false
  }
  
  /**
   * Execute workflow with progress updates
   */
  async executeWorkflow(
    nodes: CustomNode[],
    edges: Edge[],
    onProgress?: (progress: ExecutionProgress) => void
  ): Promise<ExecutionResult> {
    const startTime = Date.now()
    const results: Record<string, any> = {}
    const errors: string[] = []
    
    // Validate first
    const validation = await this.validateWorkflow(nodes, edges)
    if (!validation.valid) {
      return {
        workflowId: 'unknown',
        status: 'error',
        results: {},
        errors: validation.errors,
        executionTime: Date.now() - startTime
      }
    }
    
    // Create abort controller for cancellation
    this.abortController = new AbortController()
    
    try {
      // Get execution order (topological sort)
      const executionOrder = this.getExecutionOrder(nodes, edges)
      
      // Execute nodes in order
      for (const nodeId of executionOrder) {
        const node = nodes.find(n => n.id === nodeId)
        if (!node) continue
        
        // Update progress
        onProgress?.({
          nodeId,
          status: 'running',
          message: `Executing ${node.data.label}...`
        })
        
        try {
          // Execute node based on type
          const result = await this.executeNode(node, results, edges)
          results[nodeId] = result
          
          // Update progress
          onProgress?.({
            nodeId,
            status: 'completed',
            result,
            message: `Completed ${node.data.label}`
          })
        } catch (error) {
          const errorMessage = error instanceof Error ? error.message : 'Unknown error'
          errors.push(`Node ${node.data.label}: ${errorMessage}`)
          
          // Update progress
          onProgress?.({
            nodeId,
            status: 'error',
            message: errorMessage
          })
          
          // Continue with other nodes or stop based on error handling strategy
          if (node.type === 'output') {
            throw error // Critical node failed
          }
        }
      }
      
      return {
        workflowId: `workflow-${Date.now()}`,
        status: errors.length > 0 ? 'partial' : 'success',
        results,
        errors: errors.length > 0 ? errors : undefined,
        executionTime: Date.now() - startTime
      }
    } catch (error) {
      return {
        workflowId: 'unknown',
        status: 'error',
        results,
        errors: [...errors, error instanceof Error ? error.message : 'Unknown error'],
        executionTime: Date.now() - startTime
      }
    } finally {
      this.abortController = null
    }
  }
  
  /**
   * Get execution order using topological sort
   */
  private getExecutionOrder(nodes: CustomNode[], edges: Edge[]): string[] {
    const inDegree: Record<string, number> = {}
    const adjacencyList: Record<string, string[]> = {}
    
    // Initialize
    nodes.forEach(node => {
      inDegree[node.id] = 0
      adjacencyList[node.id] = []
    })
    
    // Build graph
    edges.forEach(edge => {
      adjacencyList[edge.source].push(edge.target)
      inDegree[edge.target]++
    })
    
    // Topological sort using Kahn's algorithm
    const queue: string[] = []
    const result: string[] = []
    
    // Find nodes with no dependencies
    Object.keys(inDegree).forEach(nodeId => {
      if (inDegree[nodeId] === 0) {
        queue.push(nodeId)
      }
    })
    
    while (queue.length > 0) {
      const nodeId = queue.shift()!
      result.push(nodeId)
      
      // Process neighbors
      adjacencyList[nodeId].forEach(neighbor => {
        inDegree[neighbor]--
        if (inDegree[neighbor] === 0) {
          queue.push(neighbor)
        }
      })
    }
    
    return result
  }
  
  /**
   * Execute a single node
   */
  private async executeNode(
    node: CustomNode,
    previousResults: Record<string, any>,
    edges: Edge[]
  ): Promise<any> {
    // Get input data from connected nodes
    const inputData = this.getNodeInputData(node.id, previousResults, edges)
    
    switch (node.type) {
      case 'input':
        return this.executeInputNode(node)
      
      case 'llm':
        return this.executeLLMNode(node, inputData)
      
      case 'compare':
        return this.executeCompareNode(node, inputData)
      
      case 'summarize':
        return this.executeSummarizeNode(node, inputData)
      
      case 'output':
        return this.executeOutputNode(node, inputData)
      
      default:
        throw new Error(`Unknown node type: ${node.type}`)
    }
  }
  
  /**
   * Get input data for a node from its connected predecessors
   */
  private getNodeInputData(
    nodeId: string,
    results: Record<string, any>,
    edges: Edge[]
  ): any[] {
    const inputEdges = edges.filter(e => e.target === nodeId)
    return inputEdges.map(edge => results[edge.source]).filter(Boolean)
  }
  
  /**
   * Execute input node
   */
  private async executeInputNode(node: CustomNode): Promise<any> {
    const data = node.data as any
    
    if (data.inputType === 'text') {
      return { type: 'text', content: data.content || '' }
    } else if (data.inputType === 'file') {
      return { type: 'file', files: data.files || [], content: data.content || '' }
    } else if (data.inputType === 'url') {
      // Fetch URL content
      try {
        const response = await fetch(`${this.baseUrl}/api/fetch-url`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ url: data.url }),
          signal: this.abortController?.signal
        })
        const result = await response.json()
        return { type: 'url', url: data.url, content: result.content }
      } catch (error) {
        return { type: 'url', url: data.url, content: '', error: error.message }
      }
    }
    
    return { type: 'unknown', content: '' }
  }
  
  /**
   * Execute LLM node - supports multiple models in parallel
   */
  private async executeLLMNode(node: CustomNode, inputData: any[]): Promise<any> {
    const data = node.data as any
    const models = data.models || []
    const prompt = data.prompt || ''
    
    // Combine input data
    const combinedInput = inputData.map(d => d.content || '').join('\n\n')
    const fullPrompt = prompt.replace('{input}', combinedInput)
    
    // If multiple models, execute in parallel
    if (models.length > 1) {
      const modelPromises = models.map((model: string) => 
        this.executeSingleModel(model, fullPrompt, data)
      )
      
      try {
        const results = await Promise.allSettled(modelPromises)
        
        // Collect successful responses
        const responses = results.map((result, index) => {
          if (result.status === 'fulfilled') {
            return result.value
          } else {
            return {
              model: models[index],
              prompt: fullPrompt,
              response: `Error: ${result.reason?.message || 'Model execution failed'}`,
              error: true,
              timestamp: new Date().toISOString()
            }
          }
        })
        
        return {
          type: 'multi-model',
          models,
          prompt: fullPrompt,
          responses,
          timestamp: new Date().toISOString()
        }
      } catch (error) {
        console.error('Multi-model execution failed:', error)
        throw error
      }
    }
    
    // Single model execution
    const model = models[0]
    if (!model) {
      return {
        error: true,
        message: 'No model selected',
        timestamp: new Date().toISOString()
      }
    }
    
    try {
      return await this.executeSingleModel(model, fullPrompt, data)
    } catch (error) {
      // Fallback to mock response for testing
      console.warn('LLM execution failed, using mock response:', error)
      return {
        model,
        prompt: fullPrompt,
        response: `Mock response for ${model}: Analyzed the input successfully.`,
        tokens: 100,
        timestamp: new Date().toISOString(),
        mock: true
      }
    }
  }
  
  /**
   * Execute a single model
   */
  private async executeSingleModel(model: string, prompt: string, config: any): Promise<any> {
    // Check if it's an Ollama model
    const isOllamaModel = model?.toLowerCase().includes('llama') || 
                         model?.toLowerCase().includes('mistral') || 
                         model?.toLowerCase().includes('codellama') ||
                         model?.toLowerCase().includes('phi') ||
                         model?.toLowerCase().includes('orca')
    
    if (isOllamaModel) {
      // Use Ollama service for local models
      const ollamaResponse = await ollamaService.generate(
        model,
        prompt,
        {
          temperature: config.temperature || 0.7,
          max_tokens: config.maxTokens || 2000
        }
      )
      
      return {
        model,
        prompt,
        response: ollamaResponse.response,
        tokens: ollamaResponse.eval_count || 0,
        timestamp: new Date().toISOString(),
        executionTime: ollamaResponse.total_duration ? ollamaResponse.total_duration / 1e9 : 0
      }
    } else {
      // Use backend API for cloud models with retry logic
      const apiKey = this.getAPIKey(model)
      const timeout = networkMonitor.getAdaptiveTimeout()
      
      const response = await fetchWithRetry(
        `${this.baseUrl}/api/llm/analyze`,
        {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            model,
            prompt,
            temperature: config.temperature || 0.5,
            max_tokens: config.maxTokens || 2000,
            api_key: apiKey
          }),
          signal: this.abortController?.signal
        },
        {
          timeout,
          retries: 3,
          onSlowNetwork: () => toast.loading(`Slow network detected. Processing ${model}...`),
          onRetry: (attempt) => toast.warning(`Retrying ${model} (attempt ${attempt}/3)`)
        }
      )
      
      if (!response.ok) {
        throw new Error(`LLM request failed: ${response.statusText}`)
      }
      
      const result = await response.json()
      return {
        model,
        prompt,
        response: result.response,
        tokens: result.tokens,
        timestamp: new Date().toISOString()
      }
    }
  }
  
  /**
   * Execute compare node
   */
  private async executeCompareNode(node: CustomNode, inputData: any[]): Promise<any> {
    const data = node.data as any
    const comparisonType = data.comparisonType || 'conflicts'
    
    // Extract responses from input data - handle both single and multi-model responses
    const responses: Array<{ model: string; response: string }> = []
    
    inputData.forEach(d => {
      if (d.type === 'multi-model' && d.responses) {
        // Handle multi-model response
        d.responses.forEach((r: any) => {
          responses.push({
            model: r.model || 'unknown',
            response: r.response || ''
          })
        })
      } else {
        // Handle single model response
        responses.push({
          model: d.model || 'unknown',
          response: d.response || d.content || ''
        })
      }
    })
    
    // Perform comparison
    const comparison = {
      type: comparisonType,
      inputs: responses,
      conflicts: [],
      consensus: [],
      differences: []
    }
    
    // Enhanced conflict detection
    if (comparisonType === 'conflicts') {
      // Analyze semantic differences between responses
      const conflicts = this.detectConflicts(responses)
      comparison.conflicts = conflicts
      
      // Add severity scoring
      if (conflicts.length > 0) {
        const uniqueResponses = new Set(responses.map(r => r.response.toLowerCase().trim()))
        const divergenceRatio = uniqueResponses.size / responses.length
        
        comparison.conflictSummary = {
          totalConflicts: conflicts.length,
          divergenceRatio,
          severity: divergenceRatio > 0.7 ? 'high' : divergenceRatio > 0.4 ? 'medium' : 'low',
          conflictingModels: [...new Set(conflicts.flatMap(c => c.models))]
        }
      }
    }
    
    // Find consensus
    if (comparisonType === 'consensus') {
      // Find common themes (simplified)
      const commonWords = this.findCommonWords(responses.map(r => r.response))
      comparison.consensus = commonWords.slice(0, 5).map(word => ({
        theme: word,
        frequency: 'high',
        models: responses.map(r => r.model)
      }))
    }
    
    return comparison
  }
  
  /**
   * Execute summarize node
   */
  private async executeSummarizeNode(node: CustomNode, inputData: any[]): Promise<any> {
    const data = node.data as any
    const length = data.length || 'medium'
    const style = data.style || 'paragraph'
    
    // Combine all input data
    const combinedContent = inputData.map(d => {
      if (d.response) return d.response
      if (d.content) return d.content
      if (d.inputs) return d.inputs.map(i => i.response).join('\n')
      return JSON.stringify(d)
    }).join('\n\n')
    
    // Create summary based on length and style
    let summary = ''
    
    if (style === 'bullets') {
      const points = combinedContent.split('.').filter(s => s.trim().length > 20)
      const numPoints = length === 'short' ? 3 : length === 'long' ? 10 : 5
      summary = points.slice(0, numPoints).map(p => `â€¢ ${p.trim()}`).join('\n')
    } else {
      const words = combinedContent.split(' ')
      const wordCount = length === 'short' ? 50 : length === 'long' ? 200 : 100
      summary = words.slice(0, wordCount).join(' ') + '...'
    }
    
    return {
      summary,
      length,
      style,
      originalLength: combinedContent.length,
      summaryLength: summary.length
    }
  }
  
  /**
   * Execute output node
   */
  private async executeOutputNode(node: CustomNode, inputData: any[]): Promise<any> {
    const data = node.data as any
    const format = data.format || 'json'
    const includeMetadata = data.includeMetadata || false
    
    let output: any = {
      timestamp: new Date().toISOString(),
      format,
      data: inputData
    }
    
    if (includeMetadata) {
      output.metadata = {
        nodeId: node.id,
        nodeLabel: node.data.label,
        executionTime: Date.now()
      }
    }
    
    // Format output based on type
    if (format === 'markdown') {
      output.formatted = this.formatAsMarkdown(inputData)
    } else if (format === 'text') {
      output.formatted = this.formatAsText(inputData)
    }
    
    return output
  }
  
  /**
   * Helper: Get API key for a model
   */
  private getAPIKey(model: string): string {
    const modelLower = model.toLowerCase()
    
    if (modelLower.includes('gpt')) {
      return localStorage.getItem('openai_api_key') || ''
    } else if (modelLower.includes('claude')) {
      return localStorage.getItem('claude_api_key') || ''
    } else if (modelLower.includes('gemini')) {
      return localStorage.getItem('gemini_api_key') || ''
    } else if (modelLower.includes('grok')) {
      return localStorage.getItem('grok_api_key') || ''
    }
    
    return ''
  }
  
  /**
   * Helper: Detect conflicts between responses
   */
  private detectConflicts(responses: Array<{ model: string; response: string }>): any[] {
    const conflicts: any[] = []
    
    // Compare each pair of responses
    for (let i = 0; i < responses.length; i++) {
      for (let j = i + 1; j < responses.length; j++) {
        const r1 = responses[i]
        const r2 = responses[j]
        
        // Check for direct contradictions
        const contradictions = this.findContradictions(r1.response, r2.response)
        if (contradictions.length > 0) {
          conflicts.push({
            type: 'contradiction',
            description: contradictions.join('; '),
            models: [r1.model, r2.model],
            severity: 'high',
            examples: contradictions
          })
        }
        
        // Check for numerical discrepancies
        const numDiscrepancies = this.findNumericalDiscrepancies(r1.response, r2.response)
        if (numDiscrepancies.length > 0) {
          conflicts.push({
            type: 'numerical_discrepancy',
            description: 'Different numerical values provided',
            models: [r1.model, r2.model],
            severity: 'medium',
            discrepancies: numDiscrepancies
          })
        }
        
        // Check for sentiment differences
        const sentimentDiff = this.compareSentiment(r1.response, r2.response)
        if (sentimentDiff.conflicting) {
          conflicts.push({
            type: 'sentiment_conflict',
            description: 'Opposing sentiments detected',
            models: [r1.model, r2.model],
            severity: 'low',
            sentiment1: sentimentDiff.sentiment1,
            sentiment2: sentimentDiff.sentiment2
          })
        }
      }
    }
    
    return conflicts
  }
  
  /**
   * Helper: Find contradictions between two texts
   */
  private findContradictions(text1: string, text2: string): string[] {
    const contradictions: string[] = []
    
    // Simple contradiction patterns
    const patterns = [
      { positive: /\b(is|are|was|were)\b/gi, negative: /\b(is not|are not|isn't|aren't|was not|were not|wasn't|weren't)\b/gi },
      { positive: /\b(can|could|will|would|should)\b/gi, negative: /\b(cannot|can't|could not|couldn't|will not|won't|would not|wouldn't|should not|shouldn't)\b/gi },
      { positive: /\b(yes|true|correct|right)\b/gi, negative: /\b(no|false|incorrect|wrong)\b/gi }
    ]
    
    patterns.forEach(pattern => {
      const hasPositive1 = pattern.positive.test(text1)
      const hasNegative1 = pattern.negative.test(text1)
      const hasPositive2 = pattern.positive.test(text2)
      const hasNegative2 = pattern.negative.test(text2)
      
      if ((hasPositive1 && hasNegative2) || (hasNegative1 && hasPositive2)) {
        contradictions.push('Contradictory assertions detected')
      }
    })
    
    return contradictions
  }
  
  /**
   * Helper: Find numerical discrepancies
   */
  private findNumericalDiscrepancies(text1: string, text2: string): any[] {
    const discrepancies: any[] = []
    
    // Extract numbers from both texts
    const numbers1 = text1.match(/\b\d+\.?\d*\b/g) || []
    const numbers2 = text2.match(/\b\d+\.?\d*\b/g) || []
    
    if (numbers1.length > 0 && numbers2.length > 0) {
      const nums1 = numbers1.map(n => parseFloat(n))
      const nums2 = numbers2.map(n => parseFloat(n))
      
      // Check for significant differences
      nums1.forEach(n1 => {
        nums2.forEach(n2 => {
          const diff = Math.abs(n1 - n2)
          const avg = (n1 + n2) / 2
          const percentDiff = avg > 0 ? (diff / avg) * 100 : 0
          
          if (percentDiff > 20) { // More than 20% difference
            discrepancies.push({
              value1: n1,
              value2: n2,
              difference: diff,
              percentDifference: percentDiff
            })
          }
        })
      })
    }
    
    return discrepancies
  }
  
  /**
   * Helper: Compare sentiment between texts
   */
  private compareSentiment(text1: string, text2: string): any {
    // Simple sentiment analysis based on keywords
    const positiveWords = /\b(good|great|excellent|positive|success|benefit|improve|better|happy|glad)\b/gi
    const negativeWords = /\b(bad|poor|negative|failure|problem|issue|worse|unhappy|sad|difficult)\b/gi
    
    const positive1 = (text1.match(positiveWords) || []).length
    const negative1 = (text1.match(negativeWords) || []).length
    const positive2 = (text2.match(positiveWords) || []).length
    const negative2 = (text2.match(negativeWords) || []).length
    
    const sentiment1 = positive1 > negative1 ? 'positive' : negative1 > positive1 ? 'negative' : 'neutral'
    const sentiment2 = positive2 > negative2 ? 'positive' : negative2 > positive2 ? 'negative' : 'neutral'
    
    return {
      sentiment1,
      sentiment2,
      conflicting: (sentiment1 === 'positive' && sentiment2 === 'negative') || 
                  (sentiment1 === 'negative' && sentiment2 === 'positive')
    }
  }
  
  /**
   * Helper: Find common words in responses
   */
  private findCommonWords(texts: string[]): string[] {
    const wordFrequency: Record<string, number> = {}
    
    texts.forEach(text => {
      const words = text.toLowerCase().split(/\s+/)
      words.forEach(word => {
        if (word.length > 4) { // Skip short words
          wordFrequency[word] = (wordFrequency[word] || 0) + 1
        }
      })
    })
    
    return Object.entries(wordFrequency)
      .sort((a, b) => b[1] - a[1])
      .map(([word]) => word)
  }
  
  /**
   * Helper: Format as Markdown
   */
  private formatAsMarkdown(data: any[]): string {
    let markdown = '# Workflow Results\n\n'
    
    data.forEach((item, index) => {
      markdown += `## Step ${index + 1}\n\n`
      
      if (item.model) {
        markdown += `**Model:** ${item.model}\n\n`
      }
      
      if (item.response) {
        markdown += `**Response:**\n${item.response}\n\n`
      } else if (item.summary) {
        markdown += `**Summary:**\n${item.summary}\n\n`
      } else if (item.conflicts) {
        markdown += `**Conflicts Found:** ${item.conflicts.length}\n\n`
      } else {
        markdown += '```json\n' + JSON.stringify(item, null, 2) + '\n```\n\n'
      }
    })
    
    return markdown
  }
  
  /**
   * Helper: Format as plain text
   */
  private formatAsText(data: any[]): string {
    return data.map((item, index) => {
      let text = `=== Step ${index + 1} ===\n`
      
      if (item.response) {
        text += item.response
      } else if (item.summary) {
        text += item.summary
      } else {
        text += JSON.stringify(item, null, 2)
      }
      
      return text
    }).join('\n\n')
  }
  
  /**
   * Cancel ongoing execution
   */
  cancelExecution() {
    if (this.abortController) {
      this.abortController.abort()
      this.abortController = null
      toast.error('Workflow execution cancelled')
    }
  }
}

// Export singleton instance
export const workflowExecutor = new WorkflowExecutor()