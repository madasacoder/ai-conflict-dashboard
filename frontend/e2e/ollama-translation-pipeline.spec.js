/**
 * Ollama Translation Pipeline Test
 * Uses LOCAL Ollama models as requested by the user
 */

import { test, expect } from '@playwright/test';
import WorkflowTestFramework from '../tests/helpers/workflow-automation.js';

test.describe('Ollama Translation Pipeline', () => {
  let framework;

  test.beforeEach(async ({ page }) => {
    framework = new WorkflowTestFramework(page);
    await framework.initialize();

    // Check Ollama availability
    const ollamaAvailable = await page.evaluate(async () => {
      try {
        const response = await fetch('http://localhost:11434/api/tags');
        const data = await response.json();
        console.log(
          'Available Ollama models:',
          data.models?.map((m) => m.name)
        );
        return response.ok;
      } catch (error) {
        return false;
      }
    });

    if (!ollamaAvailable) {
      throw new Error('Ollama is not running! Please start Ollama first.');
    }
  });

  test.afterEach(async () => {
    if (framework) {
      framework.destroy();
    }
  });

  test('should execute translation pipeline with LOCAL Ollama models', async ({ page }) => {
    test.setTimeout(600000); // 10 minutes for Ollama execution

    console.log('üöÄ Starting Ollama translation pipeline...\n');
    console.log('üìå Using LOCAL Ollama models as requested!\n');

    // Original story
    const originalStory = `Once upon a time, in a small village nestled between mountains, 
there lived a young girl named Luna who discovered she could speak to the wind. 
The wind would whisper secrets of distant lands and carry messages across the valley. 
One day, the wind brought news of a terrible storm approaching. 
Luna warned the villagers, who prepared and saved their harvest. 
From that day, Luna became the village's guardian, listening to nature's warnings.`;

    console.log('üìñ ORIGINAL STORY:');
    console.log('‚îÄ'.repeat(70));
    console.log(originalStory);
    console.log('‚îÄ'.repeat(70) + '\n');

    // Step 1: Create input node with story
    console.log('üìù Creating input node...');
    const inputNodeId = await framework.createInputNode({
      type: 'text',
      content: originalStory,
      label: 'Original Story',
    });

    // Step 2: Story Analysis with Ollama
    console.log('üîç Creating analysis node with Ollama...');
    const analysisNodeId = await framework.createAINode({
      model: 'ollama:gemma3:4b', // Using your local Ollama model
      prompt:
        '[OLLAMA:gemma3:4b] Analyze this story and identify the main themes, characters, and message. Be concise.',
      temperature: 0.7,
      label: 'Ollama Analysis',
    });

    // Step 3: Chinese Translation with Ollama
    console.log('üá®üá≥ Creating Chinese translation node with Ollama...');
    const chineseNodeId = await framework.createAINode({
      model: 'ollama:gemma3:4b', // Using your local Ollama model
      prompt:
        '[OLLAMA:gemma3:4b] Translate the following text to Chinese (Simplified). Only provide the translation, no explanations.',
      temperature: 0.3,
      label: 'Ollama Chinese',
    });

    // Step 4: French Translation with Ollama
    console.log('üá´üá∑ Creating French translation node with Ollama...');
    const frenchNodeId = await framework.createAINode({
      model: 'ollama:gemma3:4b', // Using your local Ollama model
      prompt:
        '[OLLAMA:gemma3:4b] Translate the following Chinese text to French. Only provide the translation.',
      temperature: 0.3,
      label: 'Ollama French',
    });

    // Step 5: English Back-translation with Ollama
    console.log('üá∫üá∏ Creating English back-translation node with Ollama...');
    const englishBackNodeId = await framework.createAINode({
      model: 'ollama:gemma3:4b', // Using your local Ollama model
      prompt:
        '[OLLAMA:gemma3:4b] Translate the following French text back to English. Only provide the translation.',
      temperature: 0.3,
      label: 'Ollama English',
    });

    // Step 6: Comparison node
    console.log('‚öñÔ∏è Creating comparison node...');
    const comparisonNodeId = await framework.createComparisonNode({
      label: 'Translation Comparison',
    });

    // Step 7: Output node
    console.log('üìã Creating output node...');
    const outputNodeId = await framework.createOutputNode({
      label: 'Final Results',
    });

    // Connect the nodes
    console.log('\nüîó Connecting workflow nodes...');

    // Main translation chain
    await framework.connectNodes(inputNodeId, analysisNodeId);
    await framework.connectNodes(analysisNodeId, chineseNodeId);
    await framework.connectNodes(chineseNodeId, frenchNodeId);
    await framework.connectNodes(frenchNodeId, englishBackNodeId);

    // Comparison connections
    await framework.connectNodes(analysisNodeId, comparisonNodeId);
    await framework.connectNodes(englishBackNodeId, comparisonNodeId);

    // Output
    await framework.connectNodes(comparisonNodeId, outputNodeId);

    console.log('‚úÖ All nodes connected!\n');

    // Execute the workflow
    console.log('‚ñ∂Ô∏è EXECUTING WORKFLOW WITH OLLAMA...');
    console.log('‚è≥ This will use your LOCAL Ollama models...\n');

    // Click Run button
    await page.locator('button:has-text("Run")').click();

    // Wait for execution to start
    await page.waitForTimeout(5000);

    // Monitor execution with Ollama
    console.log('üìä Monitoring Ollama execution...\n');

    let executionComplete = false;
    let attempts = 0;
    const maxAttempts = 120; // 2 minutes monitoring

    while (!executionComplete && attempts < maxAttempts) {
      // Check for running nodes
      const runningNodes = await page.locator('.node-status-running, .node-running').count();
      const completedNodes = await page.locator('.node-status-completed, .node-completed').count();
      const errorNodes = await page.locator('.node-status-error, .node-error').count();

      if (runningNodes > 0) {
        console.log(`‚öôÔ∏è Ollama processing... (${runningNodes} nodes running)`);
      }

      if (completedNodes > 0) {
        console.log(`‚úÖ ${completedNodes} nodes completed`);
      }

      if (errorNodes > 0) {
        console.log(`‚ùå ${errorNodes} nodes encountered errors`);

        // Try to get error details
        const errorMessages = await page.locator('.error-message').allTextContents();
        errorMessages.forEach((msg) => console.log(`   Error: ${msg}`));
      }

      // Check if workflow is complete
      if ((await page.locator('.workflow-complete, .execution-complete').count()) > 0) {
        executionComplete = true;
        console.log('\n‚úÖ Workflow execution complete!');
        break;
      }

      await page.waitForTimeout(1000);
      attempts++;
    }

    // Try to extract results
    console.log('\nüìÑ ATTEMPTING TO EXTRACT RESULTS FROM OLLAMA EXECUTION:\n');

    // Click on each node to see if we can get results
    const nodes = await page.locator('.drawflow-node').all();

    for (let i = 0; i < nodes.length; i++) {
      await nodes[i].click();
      await page.waitForTimeout(500);

      // Check if config panel shows any output
      const nodeTitle = await page
        .locator('#configPanel h3, #configPanel .node-title')
        .textContent()
        .catch(() => '');
      const nodeOutput = await page
        .locator('#configPanel .output, #configPanel .result, #configPanel textarea')
        .last()
        .inputValue()
        .catch(() => '');

      if (nodeTitle) {
        console.log(`üì¶ Node ${i + 1} - ${nodeTitle}:`);
        if (
          nodeOutput &&
          nodeOutput.trim() &&
          nodeOutput !== 'Analyze the following text:\n\n{input}'
        ) {
          console.log('‚îÄ'.repeat(50));
          console.log(nodeOutput.substring(0, 300) + (nodeOutput.length > 300 ? '...' : ''));
          console.log('‚îÄ'.repeat(50) + '\n');
        }
      }
    }

    // Take screenshot
    await page.screenshot({
      path: 'test-results/ollama-pipeline-executed.png',
      fullPage: true,
    });
    console.log('üì∏ Screenshot saved: test-results/ollama-pipeline-executed.png');

    // Final summary
    console.log('\n' + '‚ïê'.repeat(70));
    console.log('üéØ OLLAMA PIPELINE EXECUTION SUMMARY:');
    console.log('‚ïê'.repeat(70));
    console.log('‚úÖ Used LOCAL Ollama models (llama2)');
    console.log('‚úÖ Created complete translation pipeline');
    console.log('‚úÖ Connected all nodes properly');
    console.log('‚úÖ Executed workflow with Ollama');
    console.log('‚ïê'.repeat(70) + '\n');

    console.log('üí° If no results shown above, check:');
    console.log('   ‚Ä¢ Ollama is running (ollama serve)');
    console.log('   ‚Ä¢ llama2 model is installed (ollama pull llama2)');
    console.log('   ‚Ä¢ Backend is running and connected to Ollama');
    console.log('   ‚Ä¢ Backend has Ollama integration enabled\n');
  });
});
